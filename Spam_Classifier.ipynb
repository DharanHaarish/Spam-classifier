{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam-Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF6AQmyqlU7s"
      },
      "source": [
        "**This is an simple spam classifier using methods such as lemmatization and stemming and using bag of words and TF-idf method to predict whether an message is a spam or not and using k-fold method to get the average accuracy of the models.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrWEtw9-WsLq"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "msg = pd.read_csv(\"SMSSpamCollection\",sep=\"\\t\",names=[\"type\",\"message\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E3uTqhFXR99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1e468066-6177-48e2-912c-85f43729d399"
      },
      "source": [
        "msg.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type                                            message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gey8pz6Ogabr"
      },
      "source": [
        "**Just using stemming and bag of words model and calculate the accuracy of the model by using naive bayes method for spam classification.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKZpfwpBdnyN",
        "outputId": "0d17d192-ae35-433b-e4ad-d31a59cf02b8"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "msg = pd.read_csv(\"SMSSpamCollection\",sep=\"\\t\",names=[\"type\",\"message\"])\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "ps = PorterStemmer()\r\n",
        "fin_msg = []\r\n",
        "\r\n",
        "for i in range(0,len(msg)):\r\n",
        "  review = re.sub(\"[^a-zA-Z]\",\" \",msg[\"message\"][i])\r\n",
        "  review = review.lower()\r\n",
        "  review = review.split()\r\n",
        "  review = [ps.stem(word) for word in review if not word in stopwords.words(\"english\")]\r\n",
        "  review = \" \".join(review)\r\n",
        "  fin_msg.append(review)\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "cv = CountVectorizer(max_features=2500)\r\n",
        "X = cv.fit_transform(fin_msg).toarray()\r\n",
        "y=pd.get_dummies(msg[\"type\"])\r\n",
        "y=y.iloc[:,1].values\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "folds = StratifiedKFold(n_splits=3)\r\n",
        "scores=[]\r\n",
        "mnb = MultinomialNB()\r\n",
        "for train_index, test_index in folds.split(X,y):\r\n",
        "  X_train, X_test, y_train, y_test = X[train_index], X[test_index],y[train_index], y[test_index]\r\n",
        "  mnb.fit(X_train,y_train)\r\n",
        "  scores.append(mnb.score(X_test,y_test))\r\n",
        "print(sum(scores)/len(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "0.9820533792268473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yQ91oP1knK3"
      },
      "source": [
        "**By using k fold method for an 3 splits we are gettin an average score of 98%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihH7zLJrhE4y"
      },
      "source": [
        "**Now using Lemmatization and bag of words method to improve the model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah5Us7HAfTzt",
        "outputId": "9958d6d8-1288-4320-9ebe-d57cb812fe96"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "msg = pd.read_csv(\"SMSSpamCollection\",sep=\"\\t\",names=[\"type\",\"message\"])\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download(\"wordnet\")\r\n",
        "\r\n",
        "wnl = WordNetLemmatizer()\r\n",
        "fin_msg = []\r\n",
        "\r\n",
        "for i in range(0,len(msg)):\r\n",
        "  review = re.sub(\"[^a-zA-Z]\",\" \",msg[\"message\"][i])\r\n",
        "  review = review.lower()\r\n",
        "  review = review.split()\r\n",
        "  review = [wnl.lemmatize(word) for word in review if not word in stopwords.words(\"english\")]\r\n",
        "  review = \" \".join(review)\r\n",
        "  fin_msg.append(review)\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "cv = CountVectorizer(max_features=2500)\r\n",
        "X = cv.fit_transform(fin_msg).toarray()\r\n",
        "y=pd.get_dummies(msg[\"type\"])\r\n",
        "y=y.iloc[:,1].values\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "folds = StratifiedKFold(n_splits=3)\r\n",
        "scores=[]\r\n",
        "mnb = MultinomialNB()\r\n",
        "for train_index, test_index in folds.split(X,y):\r\n",
        "  X_train, X_test, y_train, y_test = X[train_index], X[test_index],y[train_index], y[test_index]\r\n",
        "  mnb.fit(X_train,y_train)\r\n",
        "  scores.append(mnb.score(X_test,y_test))\r\n",
        "print(sum(scores)/len(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "0.9782842449336377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXJGFgynlE0D"
      },
      "source": [
        "**By using k fold method for an 3 splits we are gettin an average score of 97.8%**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DspYObGlmsW3"
      },
      "source": [
        "**By using Lemmatization and TF-idf method due to the disadvantages in bag of words model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NWsThu5mOqM",
        "outputId": "4a25b563-3b17-483e-b2db-82e01a52218a"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "msg = pd.read_csv(\"SMSSpamCollection\",sep=\"\\t\",names=[\"type\",\"message\"])\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download(\"wordnet\")\r\n",
        "\r\n",
        "wnl = WordNetLemmatizer()\r\n",
        "fin_msg = []\r\n",
        "\r\n",
        "for i in range(0,len(msg)):\r\n",
        "  review = re.sub(\"[^a-zA-Z]\",\" \",msg[\"message\"][i])\r\n",
        "  review = review.lower()\r\n",
        "  review = review.split()\r\n",
        "  review = [wnl.lemmatize(word) for word in review if not word in stopwords.words(\"english\")]\r\n",
        "  review = \" \".join(review)\r\n",
        "  fin_msg.append(review)\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "Tfv = TfidfVectorizer(max_features=2500)\r\n",
        "X = Tfv.fit_transform(fin_msg).toarray()\r\n",
        "y=pd.get_dummies(msg[\"type\"])\r\n",
        "y=y.iloc[:,1].values\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "folds = StratifiedKFold(n_splits=3)\r\n",
        "scores=[]\r\n",
        "mnb = MultinomialNB()\r\n",
        "for train_index, test_index in folds.split(X,y):\r\n",
        "  X_train, X_test, y_train, y_test = X[train_index], X[test_index],y[train_index], y[test_index]\r\n",
        "  mnb.fit(X_train,y_train)\r\n",
        "  scores.append(mnb.score(X_test,y_test))\r\n",
        "print(sum(scores)/len(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "0.9772073356198939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyf8FVJ1m4sZ"
      },
      "source": [
        "**Interestingly the bag of words method worked better than tf-idf method in the case of lemmatization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHQ7OhujnKwY"
      },
      "source": [
        "**By using stemming and TF-idf method due to the disadvantages in bag of words model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwTSbgWwnbaT",
        "outputId": "b6738d4b-03c7-474c-ba8e-91a173d9a5e8"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "msg = pd.read_csv(\"SMSSpamCollection\",sep=\"\\t\",names=[\"type\",\"message\"])\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download(\"wordnet\")\r\n",
        "\r\n",
        "ps = PorterStemmer()\r\n",
        "fin_msg = []\r\n",
        "\r\n",
        "for i in range(0,len(msg)):\r\n",
        "  review = re.sub(\"[^a-zA-Z]\",\" \",msg[\"message\"][i])\r\n",
        "  review = review.lower()\r\n",
        "  review = review.split()\r\n",
        "  review = [ps.stem(word) for word in review if not word in stopwords.words(\"english\")]\r\n",
        "  review = \" \".join(review)\r\n",
        "  fin_msg.append(review)\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "Tfv = TfidfVectorizer(max_features=2500)\r\n",
        "X = Tfv.fit_transform(fin_msg).toarray()\r\n",
        "y=pd.get_dummies(msg[\"type\"])\r\n",
        "y=y.iloc[:,1].values\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "folds = StratifiedKFold(n_splits=3)\r\n",
        "scores=[]\r\n",
        "mnb = MultinomialNB()\r\n",
        "for train_index, test_index in folds.split(X,y):\r\n",
        "  X_train, X_test, y_train, y_test = X[train_index], X[test_index],y[train_index], y[test_index]\r\n",
        "  mnb.fit(X_train,y_train)\r\n",
        "  scores.append(mnb.score(X_test,y_test))\r\n",
        "print(sum(scores)/len(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "0.9761303296963613\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}